{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf319ae-9fa0-405f-930a-e82ade788e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('~/your_path_to_project/adp_chatbot_assistant/src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from core.model.estimator import Estimator\n",
    "from core.utils.data_modification import read_data\n",
    "from datetime import datetime\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import randint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "34fe17b8-53d0-4c8c-a98a-2d73b1e2db41",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions_logit = {\n",
    "    'classifier__penalty': ['l1', 'l2'],  # L1 and L2 regularization\n",
    "    'classifier__C': uniform(loc=0, scale=10),  # Sample C values from a uniform distribution between 0 and 10\n",
    "    'classifier__solver': ['saga'],  # Testing different solvers\n",
    "    'classifier__max_iter': [15000]  # Testing different max iterations\n",
    "}\n",
    "\n",
    "param_distributions_rf = {\n",
    "    'classifier__n_estimators': randint(100, 200),  # Randomly sample from 100 to 1000 trees\n",
    "    'classifier__max_depth': [None, 10],     # Test None (no limit) and different tree depths\n",
    "    'classifier__min_samples_split': randint(2, 20), # Randomly sample min samples for split\n",
    "    'classifier__min_samples_leaf': randint(1, 10),  # Randomly sample min samples at leaf\n",
    "    'classifier__max_features': ['sqrt', 'log2']     # sqrt or log2 features\n",
    "}\n",
    "\n",
    "param_distributions_xbg = {\n",
    "    'classifier__n_estimators': randint(300, 1500),  # Number of trees in the forest\n",
    "    'classifier__max_depth': randint(3, 10),         # Maximum depth of each tree\n",
    "    'classifier__learning_rate': uniform(0.01, 0.3), # Learning rate for boosting\n",
    "    'classifier__subsample': uniform(0.5, 0.5),      # Subsample ratio of the training instances\n",
    "    'classifier__colsample_bytree': uniform(0.5, 0.5),# Subsample ratio of columns when constructing each tree\n",
    "    'classifier__gamma': uniform(0, 10),             # Minimum loss reduction\n",
    "    'classifier__reg_alpha': uniform(0, 10),         # L1 regularization term\n",
    "    'classifier__reg_lambda': uniform(0, 10)         # L2 regularization term\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1464a197-f2a0-4ea3-9593-567e8118f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings ={\n",
    "    'logit': [LogisticRegression(random_state=1506,  class_weight='balanced'), param_distributions_logit, None], \n",
    "    'random_forest': [RandomForestClassifier(random_state=1506, warm_start=True), param_distributions_rf, None],\n",
    "    'xgb': [xgb.XGBClassifier(random_state=1506, eval_metric='mlogloss'), param_distributions_xbg, None]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ffb8ba-49ef-442e-980e-2bea673fb410",
   "metadata": {},
   "source": [
    "# TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d1bb9927-6a5f-4695-ae55-e8a624753d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eduardo/anaconda3/envs/tha_adp/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/home/eduardo/anaconda3/envs/tha_adp/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             accuracy  f1_score  precision  recall\n",
      "train_error     0.984  0.983957   0.984061   0.984\n",
      "test_error      0.950  0.950647   0.952417   0.950 \n",
      "\n",
      "random_forest\n",
      "             accuracy  f1_score  precision  recall\n",
      "train_error     0.992  0.991897   0.992123   0.992\n",
      "test_error      0.966  0.965566   0.966218   0.966 \n",
      "\n",
      "xgb\n",
      "             accuracy  f1_score  precision  recall\n",
      "train_error     0.932  0.930981   0.932083   0.932\n",
      "test_error      0.912  0.911404   0.912493   0.912 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name in settings.keys():\n",
    "    \n",
    "    print(model_name)\n",
    "    model = Estimator()\n",
    "    \n",
    "    model.set_model(classifier=settings[model_name][0])\n",
    "\n",
    "    df, topics = read_data() \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = model.train_test_split(df)\n",
    "\n",
    "    model.param_distributions = settings[model_name][1]\n",
    "    \n",
    "    model.train_with_random_search(X_train['message'], y_train, n_iter=25)\n",
    "    \n",
    "    y_train_preds = model.model_pipeline.predict_proba(X_train.message)\n",
    "    y_test_preds = model.model_pipeline.predict_proba(X_test.message)\n",
    "\n",
    "    settings[model_name][2] = model.model_pipeline\n",
    "    \n",
    "    # model.print_results_report_full(y_train_preds, y_test_preds, y_train, y_test)\n",
    "    print(pd.DataFrame(model.get_metrics(y_train_preds, y_test_preds, y_train, y_test)).rename(index={0: 'train_error',\n",
    "                                                                                          1: 'test_error'}), '\\n')\n",
    "    # print(settings[model_name][2][1].get_params())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c86f37d-0a64-443e-ab81-0705087f2b9a",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "16da5b6c-4f9b-4f9d-8057-75bb2fdfa1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_w2v ={\n",
    "    'logit': [LogisticRegression(random_state=1506,  class_weight='balanced'), param_distributions_logit, None], \n",
    "    'random_forest': [RandomForestClassifier(random_state=1506, warm_start=True), param_distributions_rf, None],\n",
    "    'xgb': [xgb.XGBClassifier(random_state=1506, eval_metric='mlogloss'), param_distributions_xbg, None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e0e5773b-198f-4af0-aa10-34543f103a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/eduardo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the required 'punkt' tokenizer data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# word2vec_model = api.load('word2vec-google-news-300')\n",
    "word2vec = api.load('glove-wiki-gigaword-50')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e16f4cab-0c13-4403-bc14-081a80441751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in remove_stopwords(words) if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "443f0e45-2e3d-4468-bfbc-bef6f7d12c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit\n",
      "             accuracy  f1_score  precision  recall\n",
      "train_error     0.697  0.698185   0.701214   0.697\n",
      "test_error      0.678  0.680977   0.685236   0.678 \n",
      "\n",
      "random_forest\n",
      "             accuracy  f1_score  precision  recall\n",
      "train_error     0.932  0.930128   0.933189   0.932\n",
      "test_error      0.698  0.685601   0.695954   0.698 \n",
      "\n",
      "xgb\n",
      "             accuracy  f1_score  precision  recall\n",
      "train_error    0.7355  0.727957   0.739336  0.7355\n",
      "test_error     0.6420  0.624648   0.627075  0.6420 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name in settings.keys():\n",
    "    \n",
    "    print(model_name)\n",
    "    model = Estimator()\n",
    "    \n",
    "    model.set_model(classifier=settings_w2v[model_name][0], word_model=MeanEmbeddingVectorizer(word2vec))\n",
    "    \n",
    "    df, topics = read_data()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = model.train_test_split(df)\n",
    "\n",
    "    model.param_distributions = settings[model_name][1]\n",
    "    \n",
    "    model.train_with_random_search(X_train['message'], y_train, n_iter=5)\n",
    "    \n",
    "    y_train_preds = model.model_pipeline.predict_proba(X_train.message)\n",
    "    y_test_preds = model.model_pipeline.predict_proba(X_test.message)\n",
    "\n",
    "    settings[model_name][2] = model.model_pipeline\n",
    "    \n",
    "     # model.print_results_report_full(y_train_preds, y_test_preds, y_train, y_test)\n",
    "    print(pd.DataFrame(model.get_metrics(y_train_preds, y_test_preds, y_train, y_test)).rename(index={0: 'train_error',\n",
    "                                                                                          1: 'test_error'}), '\\n')\n",
    "    # print(settings[model_name][2][1].get_params())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e44855-f124-48c2-8290-403a845cdfa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tha_adp",
   "language": "python",
   "name": "tha_adp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
